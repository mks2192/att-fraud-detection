{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SQLContext, HiveContext,SparkSession\n",
    "from pyspark.ml import Pipeline,PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"test-spark-streaming\")\\\n",
    ".master(\"local[*]\")\\\n",
    ".config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\\n",
    ".getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"windowsos\"\n",
    "tenant_name = \"itd\"\n",
    "time_window=\"day\"\n",
    "entity_type=\"ip\"\n",
    "anomaly_type=\"profile\"\n",
    "model_type=\"sklearn\"\n",
    "model_name=\"isolationforest\"\n",
    "\n",
    "BASE_PATH = \"/Users/tuhinsharma/Documents/sstech/\"+tenant_name\n",
    "ANOMALY_DATA_REPOSITORY = BASE_PATH + \"/models_data/data\"\n",
    "\n",
    "USER_PROFILE_DATA_PATH = ANOMALY_DATA_REPOSITORY + \"/{data_source}/{entity_type}/{anomaly_type}/{time_window}.json\"\n",
    "data_path = USER_PROFILE_DATA_PATH.format\\\n",
    "                                           (data_source=data_source,\\\n",
    "                                            entity_type=entity_type,anomaly_type=\"profile\",time_window=time_window)\n",
    "    \n",
    "ANOMALY_MODEL_REPOSITORY = BASE_PATH + \"/models_data/model\"\n",
    "PROFILE_ANOMALY_MODEL_PATH = ANOMALY_MODEL_REPOSITORY + \"/{data_source}/{entity_type}/{anomaly_type}/{time_window}/{model_type}/{model_name}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = PROFILE_ANOMALY_MODEL_PATH.format(data_source=data_source,\\\n",
    "                                  entity_type=entity_type,anomaly_type=anomaly_type,time_window=time_window,\\\n",
    "                                 model_type=model_type,model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def binary_to_dict(x):\n",
    "    my_dict = json.loads(x)\n",
    "    return my_dict[\"dns_error_count\"]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-03-26 22:55:10\n",
      "-------------------------------------------\n",
      "3904\n",
      "3906\n",
      "3908\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-03-26 22:55:15\n",
      "-------------------------------------------\n",
      "3910\n",
      "3912\n",
      "3914\n",
      "3916\n",
      "3918\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-03-26 22:55:20\n",
      "-------------------------------------------\n",
      "3920\n",
      "3922\n",
      "3924\n",
      "3926\n",
      "3928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 5)\n",
    "brokers, topic = \"localhost:9092\", \"mytesttopic\"\n",
    "kvs = KafkaUtils.createDirectStream(ssc, [topic],{\"metadata.broker.list\": brokers})\n",
    "lines = kvs.map(lambda x: x[1]).map(binary_to_dict)\n",
    "\n",
    "lines.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = '/tmp/checkpoint'\n",
    "\n",
    "def getSqlContextInstance(sparkContext):\n",
    "\tif ('sqlContextSingletonInstance' not in globals()):\n",
    "\t\tglobals()['sqlContextSingletonInstance'] = HiveContext(sparkContext)\n",
    "\treturn globals()['sqlContextSingletonInstance']\n",
    "\n",
    "def createContext():\n",
    "\tsc = SparkContext(appName=\"PythonStreamingKafkaChCount\")\n",
    "\tssc = StreamingContext(sc, 5)\n",
    "\tzkQuorum, topic = sys.argv[1:]\n",
    "\tkvs = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})\n",
    "\tparsed = kvs.map(lambda k,v: json.loads(v))\n",
    "\ttimes_dstream = parsed.map(lambda x: x['time'])\n",
    "\tchannel_dstream = parsed.map(lambda x: x['data'])\n",
    "\n",
    "\tcount_values_windowed = channel_dstream.flatMap(lambda x: x.items())\\\n",
    "\t\t.reduceByKeyAndWindow(lambda x,y: x+y, lambda x, y:x-y, 30,5)\n",
    "\n",
    "\tdef writeRecord(time, rdd):\n",
    "\t\ttry:\n",
    "\t\t\thiveContext = getSqlContextInstance(rdd.context)\n",
    "\t\t\trowRdd = rdd.map(lambda x: Row(time=time, key=x[0], value=x[1]))\n",
    "\t\t\tprint(rowRdd.take(5))\n",
    "\t\t\twordsDataFrame = hiveContext.createDataFrame(rowRdd)\n",
    "\t\t\twordsDataFrame.show()\n",
    "\t\t\twordsDataFrame.registerTempTable(\"wc\")\n",
    "\t\t\twordsDataFrame = hiveContext.sql(\"INSERT INTO TABLE word_count SELECT time, key, value from wc\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(str(e))\n",
    "\t\t\tpass\n",
    "\n",
    "\tcount_values_windowed.foreachRDD(writeRecord)\n",
    "\treturn ssc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tif len(sys.argv) != 3:\n",
    "\t\tprint(\"Usage: [filename].py <zk> <topic>\", file=sys.stderr)\n",
    "\t\texit(-1)\n",
    "\telse:\n",
    "\t\tprint(\"Creating new context\")\n",
    "\t\tif os.path.exists(outputPath):\n",
    "\t\t\tos.remove(outputPath)\n",
    "\n",
    "\t\tssc = StreamingContext.getOrCreate(outputPath, lambda: createContext())\n",
    "\t\tssc.start()\n",
    "\t\tssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sstech-models-v3",
   "language": "python",
   "name": "sstech-models-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
